{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NVIDIA TAO DetectNet_v2 for Infineon PSOC&trade; Edge MCU devices\n",
    "\n",
    "This comprehensive guide on training and optimizing neural networks for deployment on Infineon PSOC&trade; Edge MCU devices using NVIDIA's Train Adapt Optimize (TAO) Toolkit. This notebook guides you through the complete workflow from training a custom object detection model to optimizing it for efficient deployment on resource-constrained edge devices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is TAO Toolkit?\n",
    "\n",
    "Transfer learning is the process of transferring learned features from one application to another. It is a commonly used training technique where you use a model trained on one task and retrain to use it on a different task. \n",
    "\n",
    "Train Adapt Optimize (TAO) Toolkit is a simple and easy-to-use Python-based AI toolkit for taking purpose-built AI models and customizing them with your own data. The TAO Toolkit provides a streamlined workflow to:\n",
    "\n",
    "- **Train:** Fine-tune pretrained models with your own data\n",
    "- **Adapt:** Adapt models to your specific use case\n",
    "- **Optimize:** Optimize models for efficient deployment on edge devices\n",
    "\n",
    "<img align=\"center\" src=\"https://d29g4g2dyqv443.cloudfront.net/sites/default/files/akamai/TAO/tlt-tao-toolkit-bring-your-own-model-diagram.png\" width=\"1080\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is DetectNet-V2?\n",
    "\n",
    "DetectNet_v2 is also known as \"GridBox object detection\" that is a highly optimized CNN-based object detection model designed for efficient inference, which works by:\n",
    "\n",
    "- Dividing an input image into a uniform grid\n",
    "- Predicting four normalized bounding-box parameters (xc, yc, w, h) and confidence value per output class for each grid cell\n",
    "- Post-processing detections using clustering algorithms such as DBSCAN, NMS, or HYBRID (DBSCAN + NMS)\n",
    "\n",
    "DetectNet_v2 is well-suited for resource-constrained environments like the Infineon PSOC&trade; Edge MCU devices due to its computational efficiency while maintaining high accuracy.\n",
    "\n",
    "### Sample output predictions from a trained DetectNet_v2 model\n",
    "\n",
    "<img align=\"center\" src=\"https://miro.medium.com/v2/resize:fit:720/0*YaQDIKR4gRbP2-by\" width=\"960\">\n",
    "\n",
    "<img align=\"center\" src=\"https://miro.medium.com/v2/resize:fit:720/0*eY1qluSyldYl9qDw\" width=\"960\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow overview and learning objectives\n",
    "\n",
    "This notebook demonstrates how to leverage the NVIDIA TAO Toolkit to train and optimize a DetectNet_v2 model for deployment on Infineon PSOC&trade; Edge MCU devices. It provides:\n",
    "\n",
    "- **Environment setup:** Configure your environment for TAO Toolkit\n",
    "- **Data preparation:** Prepare your dataset in the required format for training\n",
    "- **Model training:** Train a ResNet-18 based DetectNet_v2 model on the COCO dataset\n",
    "- **Model evaluation:** Evaluate the trained model's performance\n",
    "- **Model pruning:** Optimize the model size by pruning unnecessary weights\n",
    "- **Model retraining:** Recover accuracy after pruning through retraining\n",
    "- **Quantization-aware training (QAT):** Further optimize the model for deployment on edge devices\n",
    "- **Model export:** Export the optimized model to ONNX format for deployment\n",
    "- **Infineon software integration:** Convert the model for Infineon PSOC&trade; Edge MCU devices\n",
    "\n",
    "At the end of this notebook, you will have a trained, pruned, quantized, and deployment-ready object detection model optimized for Infineon PSOC&trade; Edge MCU devices."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment setup\n",
    "\n",
    "Set up your environment variables and configure the workspace. These variables will be used throughout the notebook to reference directories and files.\n",
    "\n",
    "### 1.1 Setting environment variables\n",
    "\n",
    "The following cell set up environment variables that define:\n",
    "- The number of GPUs to use for training\n",
    "- Directory paths for experiments, data, and specifications\n",
    "\n",
    "> **Note:** Ensure to remove any stray artifacts/files from previous experiments in `$USER_EXPERIMENT_DIR` or `$DATA_DOWNLOAD_DIR` as they can interfere with creating a training graph for a new experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up env variables for cleaner command line commands.\n",
    "import os\n",
    "\n",
    "%env NUM_GPUS=2\n",
    "%env USER_EXPERIMENT_DIR=/workspace/tao-experiments/detectnet_v2\n",
    "%env DATA_DOWNLOAD_DIR=/workspace/tao-experiments/data\n",
    "\n",
    "# Set this path if you don't run the notebook from the samples directory.\n",
    "# %env NOTEBOOK_ROOT=~/tao-samples/detectnet_v2\n",
    "\n",
    "# Please define this local project directory that needs to be mapped to the TAO docker session.\n",
    "# The dataset expected to be present in $LOCAL_PROJECT_DIR/data, while the results for the steps\n",
    "# in this notebook will be stored at $LOCAL_PROJECT_DIR/detectnet_v2\n",
    "# !PLEASE MAKE SURE TO UPDATE THIS PATH!.\n",
    "\n",
    "os.environ[\"LOCAL_PROJECT_DIR\"] = FIXME\n",
    "\n",
    "os.environ[\"LOCAL_DATA_DIR\"] = os.path.join(\n",
    "    os.getenv(\"LOCAL_PROJECT_DIR\", os.getcwd()),\n",
    "    \"data\"\n",
    ")\n",
    "os.environ[\"LOCAL_EXPERIMENT_DIR\"] = os.path.join(\n",
    "    os.getenv(\"LOCAL_PROJECT_DIR\", os.getcwd()),\n",
    "    \"detectnet_v2\"\n",
    ")\n",
    "\n",
    "# Make the experiment directory \n",
    "! mkdir -p $LOCAL_EXPERIMENT_DIR\n",
    "\n",
    "# The sample spec files are present in the same path as the downloaded samples.\n",
    "os.environ[\"LOCAL_SPECS_DIR\"] = os.path.join(\n",
    "    os.getenv(\"NOTEBOOK_ROOT\", os.getcwd()),\n",
    "    \"specs\"\n",
    ")\n",
    "%env SPECS_DIR=/workspace/tao-experiments/detectnet_v2/specs\n",
    "CLEARML_LOGGED_IN = False\n",
    "WANDB_LOGGED_IN = False\n",
    "\n",
    "# Showing list of specification files.\n",
    "!ls -rlt $LOCAL_SPECS_DIR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Mapping directories to TAO docker\n",
    "\n",
    "The following cell maps local directories to the TAO docker container, allowing the TAO Toolkit to access data and save the results.\n",
    "\n",
    "The mapping includes:\n",
    "- Data directory containing dataset\n",
    "- Specifications directory containing configuration files\n",
    "- Experiment directory where outputs will be stored\n",
    "\n",
    "This step ensures the proper data access and result persistence between your local filesystem and the TAO docker environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping up the local directories to the TAO docker.\n",
    "import json\n",
    "mounts_file = os.path.expanduser(\"~/.tao_mounts.json\")\n",
    "\n",
    "# Define the dictionary with the mapped drives\n",
    "drive_map = {\n",
    "    \"Mounts\": [\n",
    "        # Mapping the data directory\n",
    "        {\n",
    "            \"source\": os.environ[\"LOCAL_PROJECT_DIR\"],\n",
    "            \"destination\": \"/workspace/tao-experiments\"\n",
    "        },\n",
    "        # Mapping the specs directory.\n",
    "        {\n",
    "            \"source\": os.environ[\"LOCAL_SPECS_DIR\"],\n",
    "            \"destination\": os.environ[\"SPECS_DIR\"]\n",
    "        },\n",
    "        {\n",
    "            \"source\": \"/teamspace/studios/this_studio/data\",\n",
    "            \"destination\": \"/workspace/tao-experiments/data\"\n",
    "        }\n",
    "    ],\n",
    "    \"DockerOptions\":{\n",
    "        \"user\": f\"{os.getuid()}:{os.getgid()}\"\n",
    "    }\n",
    "}\n",
    "\n",
    "if CLEARML_LOGGED_IN:\n",
    "    if \"Envs\" not in drive_map.keys():\n",
    "        drive_map[\"Envs\"] = []\n",
    "    drive_map[\"Envs\"].extend([\n",
    "        {\n",
    "            \"variable\": \"CLEARML_WEB_HOST\",\n",
    "            \"value\": os.getenv(\"CLEARML_WEB_HOST\")\n",
    "        },\n",
    "        {\n",
    "            \"variable\": \"CLEARML_API_HOST\",\n",
    "            \"value\": os.getenv(\"CLEARML_API_HOST\")\n",
    "        },\n",
    "        {\n",
    "            \"variable\": \"CLEARML_FILES_HOST\",\n",
    "            \"value\": os.getenv(\"CLEARML_FILES_HOST\")\n",
    "        },\n",
    "        {\n",
    "            \"variable\": \"CLEARML_API_ACCESS_KEY\",\n",
    "            \"value\": os.getenv(\"CLEARML_API_ACCESS_KEY\")\n",
    "        },\n",
    "        {\n",
    "            \"variable\": \"CLEARML_API_SECRET_KEY\",\n",
    "            \"value\": os.getenv(\"CLEARML_API_SECRET_KEY\")\n",
    "        },\n",
    "    ])\n",
    "\n",
    "if WANDB_LOGGED_IN:\n",
    "    if \"Envs\" not in drive_map.keys():\n",
    "        drive_map[\"Envs\"] = []\n",
    "    # Weights and biases currently requires access to the\n",
    "    # /.config directory in the docker. Therefore, the docker\n",
    "    # must be instantiated as root user. With the cells mentioned below\n",
    "    # we will be deleting the cells that set user roles.\n",
    "    if \"user\" in drive_map[\"DockerOptions\"].keys():\n",
    "        del(drive_map[\"DockerOptions\"][\"user\"])\n",
    "    drive_map[\"Envs\"].extend([\n",
    "        {\n",
    "            \"variable\": \"WANDB_API_KEY\",\n",
    "            \"value\": os.getenv(\"WANDB_API_KEY\")\n",
    "        }\n",
    "    ])\n",
    "\n",
    "# Writing the mounts file.\n",
    "with open(mounts_file, \"w\") as mfile:\n",
    "    json.dump(drive_map, mfile, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!cat ~/.tao_mounts.json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Installing the TAO Launcher\n",
    "\n",
    "The TAO launcher is a Python package that provides the interface to the TAO toolkit. It's distributed as a Python wheel on PyPI and can be installed with pip.\n",
    "\n",
    "**Prerequisites**:\n",
    "- Python >=3.7, <=3.10.x\n",
    "- docker-ce > 19.03.5\n",
    "- docker-API 1.40\n",
    "- nvidia-container-toolkit > 1.3.0-1\n",
    "- nvidia-container-runtime > 3.4.0-1\n",
    "- nvidia-docker2 > 2.5.0-1\n",
    "- nvidia-driver > 455+\n",
    "\n",
    "After installation, we verify the TAO launcher version to ensure everything is set up correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKIP this step IF you have already installed the TAO launcher wheel.\n",
    "!pip3 install nvidia-tao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the versions of the TAO launcher\n",
    "!tao info --verbose"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare dataset and pretrained model <a class=\"anchor\" id=\"head-2\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Data preparation\n",
    "\n",
    "This section provides the preparation of the COCO dataset for training the object detection model. Structure the dataset and convert it to TFRecords format for efficient training.\n",
    "\n",
    "### 2.1.1 Downloading the dataset\n",
    "\n",
    "Download the COCO dataset that contains images and annotations for common objects in context. This dataset is widely used for object detection tasks and provides a good baseline for training models that can later be fine-tuned for specific applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create local dir\n",
    "!mkdir -p $LOCAL_DATA_DIR\n",
    "!mkdir -p $LOCAL_EXPERIMENT_DIR\n",
    "# Download and preprocess data\n",
    "!tao model detectnet_v2 run bash $SPECS_DIR/download_coco.sh $DATA_DOWNLOAD_DIR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Verifying the downloaded dataset\n",
    "\n",
    "Verify that the dataset has been correctly downloaded by checking the number of training images, labels, and testing images. This step ensures all the necessary data before proceeding to model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify\n",
    "import os\n",
    "\n",
    "DATA_DIR = os.environ.get('LOCAL_DATA_DIR')\n",
    "num_training_images = len(os.listdir(os.path.join(DATA_DIR, \"raw-data/train2017\")))\n",
    "num_val_images = len(os.listdir(os.path.join(DATA_DIR, \"raw-data/val2017\")))\n",
    "print(\"Number of images in the train set. {}\".format(num_training_images))\n",
    "print(\"Number of images in the val set. {}\".format(num_val_images))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Converting dataset to TFRecords\n",
    "\n",
    "For efficient training with the NVIDIA TAO Toolkit, convert the downloaded dataset to TFRecords format. TFRecords is the native file format for TensorFlow, optimized for handling large datasets and enabling faster data loading during training and involves:\n",
    "- Reviewing the TFRecords conversion specification file\n",
    "- Creating a directory for storing the TFRecords\n",
    "- Running the conversion process\n",
    "- Verifying the generated TFRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TFrecords conversion spec file for kitti training\")\n",
    "!cat $LOCAL_SPECS_DIR/detectnet_v2_tfrecords_coco_trainval.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new directory for the output tfrecords dump.\n",
    "print(\"Converting Tfrecords for kitti trainval dataset\")\n",
    "!mkdir -p $LOCAL_DATA_DIR/tfrecords\n",
    "!tao model detectnet_v2 dataset_convert \\\n",
    "                  -d $SPECS_DIR/detectnet_v2_tfrecords_coco_trainval.txt \\\n",
    "                  -o $DATA_DOWNLOAD_DIR/tfrecords/coco_trainval/coco_trainval \\\n",
    "                  -r $USER_EXPERIMENT_DIR/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -rlt $LOCAL_DATA_DIR/tfrecords/coco_trainval/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao model detectnet_v2 run bash $SPECS_DIR/check_data.sh"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Downloading the pretrained model\n",
    "\n",
    "To leverage transfer learning, download a pretrained ResNet-18 model from NVIDIA's NGC registry. Using a pretrained model reduces training time and leads to better performance, especially when training data is limited.\n",
    "\n",
    "For DetectNet_v2, the input is expected to be 0 to 1 normalized with input channels in RGB order. Therefore, use models from the `nvidia/tao/pretrained_detectnet_v2` repository which are specifically prepared for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing NGC CLI on the local machine.\n",
    "## Download and install\n",
    "%env CLI=ngccli_cat_linux.zip\n",
    "!mkdir -p $LOCAL_PROJECT_DIR/ngccli\n",
    "\n",
    "# Remove any previously existing CLI installations\n",
    "!rm -rf $LOCAL_PROJECT_DIR/ngccli/*\n",
    "!wget \"https://ngc.nvidia.com/downloads/$CLI\" -P $LOCAL_PROJECT_DIR/ngccli\n",
    "!unzip -u \"$LOCAL_PROJECT_DIR/ngccli/$CLI\" -d $LOCAL_PROJECT_DIR/ngccli/\n",
    "!rm $LOCAL_PROJECT_DIR/ngccli/*.zip \n",
    "os.environ[\"PATH\"]=\"{}/ngccli/ngc-cli:{}\".format(os.getenv(\"LOCAL_PROJECT_DIR\", \"\"), os.getenv(\"PATH\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List models available in the model registry.\n",
    "!ngc registry model list nvidia/tao/pretrained_detectnet_v2:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the target destination to download the model.\n",
    "!mkdir -p $LOCAL_EXPERIMENT_DIR/pretrained_resnet18/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the pretrained model from NGC\n",
    "!ngc registry model download-version nvidia/tao/pretrained_detectnet_v2:resnet18 \\\n",
    "    --dest $LOCAL_EXPERIMENT_DIR/pretrained_resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -rlt $LOCAL_EXPERIMENT_DIR/pretrained_resnet18/pretrained_detectnet_v2_vresnet18"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training configuration\n",
    "\n",
    "Configure the training parameters with the dataset and pretrained model. The training specification file defines various aspects of the training process that includes:\n",
    "\n",
    "- Dataset paths and configurations\n",
    "- Pretrained model path\n",
    "- Augmentation parameters for data diversification\n",
    "- Training hyperparameters (batch size, learning rate, epochs, etc.)\n",
    "- Evaluation settings\n",
    "\n",
    "Observe the training specification file to understand the configuration for the DetectNet_v2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat $LOCAL_SPECS_DIR/detectnet_v2_train_resnet18_coco.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model training\n",
    "\n",
    "With the environment set up, data prepared, and training configuration defined, now train the DetectNet_v2 model. This process will fine-tune the pretrained ResNet-18 model on the specific dataset.\n",
    "\n",
    "**Notes:**\n",
    "- Training can take several hours to complete depending on your GPU configuration\n",
    "- DetectNet_v2 supports restart from checkpoint if training is interrupted\n",
    "- When using multiple GPUs, you need to adjust batch size and learning rate accordingly\n",
    "\n",
    "Firstly, ensure that the experiment directory is clean by removing any previous training artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf $USER_EXPERIMENT_DIR/experiment_dir_unpruned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Running the training process\n",
    "\n",
    "Start the training process using the TAO Toolkit as follows:\n",
    "- Specifies the model type (detectnet_v2)\n",
    "- Provides the training specification file\n",
    "- Sets the output directory for the trained model\n",
    "- Names the model for easier reference\n",
    "- Specifies the number of GPUs to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao model detectnet_v2 train -e $SPECS_DIR/detectnet_v2_train_resnet18_coco.txt \\\n",
    "                        -r $USER_EXPERIMENT_DIR/experiment_dir_unpruned \\\n",
    "                        -n resnet18_detector \\\n",
    "                        --gpus $NUM_GPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model for each epoch:')\n",
    "print('---------------------')\n",
    "!ls -lh $LOCAL_EXPERIMENT_DIR/experiment_dir_unpruned/weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model evaluation\n",
    "\n",
    "After training the model, evaluate its performance to understand how well it performs on the validation dataset. As it gives a baseline metric before proceeding with the optimization steps.\n",
    "\n",
    "The evaluation process calculates key metrics, such as precision, recall, and mean average precision (mAP) to quantify the model's accuracy in detecting objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao model detectnet_v2 evaluate -e $SPECS_DIR/detectnet_v2_train_resnet18_coco.txt \\\n",
    "                           -m $USER_EXPERIMENT_DIR/experiment_dir_unpruned/weights/resnet18_detector.hdf5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model pruning\n",
    "\n",
    "Pruning is a model optimization technique that removes unnecessary weights from the neural network, resulting in a smaller model with faster inference times. It is important for deployment on resource-constrained devices like Infineon PSOC&trade; Edge MCU.\n",
    "\n",
    "### 6.1 Pruning parameters\n",
    "\n",
    "The key parameters for pruning are as follows:\n",
    "\n",
    "- **Pruning threshold (`-pth`):** Controls the trade-off between model size and accuracy. Higher values result in smaller models but potentially lower accuracy\n",
    "- **Equalization layers (`-el`):** Specifies layers considered for pruning\n",
    "\n",
    "> **Note:** The optimal pruning threshold depends on your specific dataset and requirements. A value of 0.01 is often a good starting point for DetectNet_v2 models, but you need to experiment to find the best balance between model size and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an output directory if it doesn't exist.\n",
    "!mkdir -p $LOCAL_EXPERIMENT_DIR/experiment_dir_pruned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Performing initial pruning\n",
    "\n",
    "Perform an initial pruning step targeting specific layers of the ResNet-18 model with a pruning threshold of 0.9, which removes a significant portion of the model weights while preserving the most important ones for maintaining accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao model detectnet_v2 prune \\\n",
    "                  -m $USER_EXPERIMENT_DIR/experiment_dir_unpruned/weights/resnet18_detector.hdf5 \\\n",
    "                  -o $USER_EXPERIMENT_DIR/experiment_dir_pruned/resnet18_nopool_bn_detectnet_v2_pruned.hdf5 \\\n",
    "                  -pth 0.9 \\\n",
    "                  -el block_1b_conv_1 block_1b_conv_2 \\\n",
    "                    block_1b_conv_shortcut block_1c_conv_1 block_1c_conv_2 \\\n",
    "                    block_1c_conv_shortcut block_2a_conv_1 block_2a_conv_2 \\\n",
    "                    block_2a_conv_shortcut block_2b_conv_1 block_2b_conv_2 \\\n",
    "                    block_2b_conv_shortcut block_2c_conv_1 block_2c_conv_2 \\\n",
    "                    block_2c_conv_shortcut block_2d_conv_1 block_2d_conv_2 \\\n",
    "                    block_2d_conv_shortcut block_3a_conv_1 block_3a_conv_2 \\\n",
    "                    block_3a_conv_shortcut block_3b_conv_1 block_3b_conv_2 \\\n",
    "                    block_3b_conv_shortcut block_3c_conv_1 block_3c_conv_2 \\\n",
    "                    block_3c_conv_shortcut block_3d_conv_1 block_3d_conv_2 \\\n",
    "                    block_3d_conv_shortcut block_3e_conv_1 block_3e_conv_2 \\\n",
    "                    block_3e_conv_shortcut block_3f_conv_1 block_3f_conv_2 \\\n",
    "                    block_3f_conv_shortcut  block_4a_conv_1 block_4b_conv_1 \\\n",
    "                    block_4c_conv_1 block_4a_conv_2 block_4b_conv_2 \\\n",
    "                    block_4c_conv_2 block_4a_conv_shortcut \\\n",
    "                    block_4b_conv_shortcut block_4c_conv_shortcut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -rlt $LOCAL_EXPERIMENT_DIR/experiment_dir_pruned/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Performing additional pruning\n",
    "\n",
    "To reduce the model size, apply a second pruning step to the model, targeting additional layers with a pruning threshold of 0.8, which is a progressive pruning approach that helps maintain a better balance between model size and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao model detectnet_v2 prune \\\n",
    "                  -m $USER_EXPERIMENT_DIR/experiment_dir_pruned/resnet18_nopool_bn_detectnet_v2_pruned.hdf5 \\\n",
    "                  -o $USER_EXPERIMENT_DIR/experiment_dir_pruned/resnet18_nopool_bn_detectnet_v2_pruned_final.hdf5 \\\n",
    "                  -pth 0.8 \\\n",
    "                  -el -el conv1 block_1a_conv_1 block_1a_conv_2 block_1a_conv_shortcut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -rlt $LOCAL_EXPERIMENT_DIR/experiment_dir_pruned/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model retraining\n",
    "\n",
    "After pruning, the model typically experiences some accuracy degradation. To recover this lost accuracy, retrain the pruned model, where the retraining process preserves the smaller model size while improving its performance.\n",
    "\n",
    "### 7.1 Configuring retraining\n",
    "\n",
    "For retraining:\n",
    "- Use the pruned model as the starting point\n",
    "- Set `load_graph` to `true` in the model configuration to load the pruned model's architecture\n",
    "- Adjust training parameters if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the retrain experiment file. \n",
    "# Note: We have updated the experiment file to include the \n",
    "# newly pruned model as a pretrained weights and, the\n",
    "# load_graph option is set to true \n",
    "!cat $LOCAL_SPECS_DIR/detectnet_v2_retrain_resnet18_coco.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Running retraining\n",
    "\n",
    "Now retrain the pruned model to recover accuracy, which typically requires a less epoch than the initial training because of the fine-tuning rather than training from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retraining using the pruned model as pretrained weights \n",
    "!tao model detectnet_v2 train -e $SPECS_DIR/detectnet_v2_retrain_resnet18_coco.txt \\\n",
    "                        -r $USER_EXPERIMENT_DIR/experiment_dir_retrain \\\n",
    "                        -n resnet18_detector_pruned \\\n",
    "                        --gpus $NUM_GPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing the newly retrained model.\n",
    "!ls -rlt $LOCAL_EXPERIMENT_DIR/experiment_dir_retrain/weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluating the retrained model\n",
    "\n",
    "After retraining, evaluate the pruned and retrained model to ensure that it has recovered the accuracy lost during pruning, validating that the optimized model maintains acceptable performance for the use case."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `evaluate` command for the pruned and retrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao model detectnet_v2 evaluate -e $SPECS_DIR/detectnet_v2_retrain_resnet18_coco.txt \\\n",
    "                           -m $USER_EXPERIMENT_DIR/experiment_dir_retrain/weights/resnet18_detector_pruned.hdf5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualizing inferences\n",
    "\n",
    "To get a qualitative model's performance, run inference on test images and visualize the detection results, which helps to identify any specific weaknesses or strengths in the model's detection capabilities.\n",
    "\n",
    "### 9.1 Preparing test samples\n",
    "\n",
    "Primarily select a subset of images from the validation set to use for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $LOCAL_DATA_DIR/test_samples\n",
    "!cp $(ls $LOCAL_DATA_DIR/raw-data/val2017/* | head -n 20) $LOCAL_DATA_DIR/test_samples/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Running inference\n",
    "\n",
    "Run inference on test samples using the trained model, which:\n",
    "- Processes each image in the test directory\n",
    "- Generates bounding box predictions using our trained DetectNet_v2 model\n",
    "- Saves annotated images and detection labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running inference for detection on n images\n",
    "!tao model detectnet_v2 inference -e $SPECS_DIR/detectnet_v2_inference_coco.txt \\\n",
    "                            -r $USER_EXPERIMENT_DIR/tlt_infer_testing \\\n",
    "                            -i $DATA_DOWNLOAD_DIR/test_samples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `inference` tool produces two outputs. \n",
    "- Overlain images in `$USER_EXPERIMENT_DIR/tlt_infer_testing/images_annotated`\n",
    "- Frame by frame box labels in KITTI format located in `$USER_EXPERIMENT_DIR/tlt_infer_testing/labels`\n",
    "\n",
    "> **Note:** To run inferences for a single image, replace the path to the -i flag in the `inference` command with the path to the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Visualizing results\n",
    "\n",
    "The inference process produces two types of outputs:\n",
    "- Annotated images with bounding boxes in `$USER_EXPERIMENT_DIR/tlt_infer_testing/images_annotated`\n",
    "- Detection labels in KITTI format in `$USER_EXPERIMENT_DIR/tlt_infer_testing/labels`\n",
    "\n",
    "Create a function to visualize these results in a grid as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple grid visualizer\n",
    "!pip3 install \"matplotlib>=3.3.3, <4.0\"\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from math import ceil\n",
    "valid_image_ext = ['.jpg', '.png', '.jpeg', '.ppm']\n",
    "\n",
    "def visualize_images(image_dir, num_cols=4, num_images=10):\n",
    "    output_path = os.path.join(os.environ['LOCAL_EXPERIMENT_DIR'], image_dir)\n",
    "    num_rows = int(ceil(float(num_images) / float(num_cols)))\n",
    "    f, axarr = plt.subplots(num_rows, num_cols, figsize=[80,30])\n",
    "    f.tight_layout()\n",
    "    a = [os.path.join(output_path, image) for image in os.listdir(output_path) \n",
    "         if os.path.splitext(image)[1].lower() in valid_image_ext]\n",
    "    for idx, img_path in enumerate(a[:num_images]):\n",
    "        col_id = idx % num_cols\n",
    "        row_id = idx // num_cols\n",
    "        img = plt.imread(img_path)\n",
    "        axarr[row_id, col_id].imshow(img) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It displays the primary 12 annotated images to visually inspect the model's detection performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the first 12 images.\n",
    "OUTPUT_PATH = 'tlt_infer_testing/images_annotated' # relative path from $USER_EXPERIMENT_DIR.\n",
    "COLS = 4 # number of columns in the visualizer grid.\n",
    "IMAGES = 12 # number of images to visualize.\n",
    "\n",
    "visualize_images(OUTPUT_PATH, num_cols=COLS, num_images=IMAGES)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model export\n",
    "\n",
    "For deployment on Infineon PSOC&trade; Edge MCU devices, export the trained and optimized model to Open Neural Network Exchange (ONNX) format, which is an open format for representing deep learning models that enables interoperability between different frameworks.\n",
    "\n",
    "In this step, export the pruned and retrained model to ONNX format using the TensorFlow-to-ONNX converter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $LOCAL_EXPERIMENT_DIR/experiment_dir_final\n",
    "# Removing a pre-existing copy of the onnx if there has been any.\n",
    "import os\n",
    "output_file=os.path.join(os.environ['LOCAL_EXPERIMENT_DIR'],\n",
    "                         \"experiment_dir_final/resnet18_detector.onnx\")\n",
    "if os.path.exists(output_file):\n",
    "    os.system(\"rm {}\".format(output_file))\n",
    "!tao model detectnet_v2 export \\\n",
    "                  -m $USER_EXPERIMENT_DIR/experiment_dir_retrain/weights/resnet18_detector_pruned.hdf5 \\\n",
    "                  -e $SPECS_DIR/detectnet_v2_retrain_resnet18_coco.txt \\\n",
    "                  -o $USER_EXPERIMENT_DIR/experiment_dir_final/resnet18_detector.onnx \\\n",
    "                  --onnx_route tf2onnx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Quantization-aware training (QAT)\n",
    "\n",
    "Quantization is a technique to reduce model size and improve inference speed by representing weights and activations with lower precision (e.g., INT8 instead of FP32). It is important for deployment on resource-constrained edge devices like Infineon PSOC&trade; Edge MCU.\n",
    "\n",
    "While post-training quantization is simple, it often leads to accuracy degradation. Quantization-aware training (QAT) addresses it by simulating quantization effects during training, allowing the model to adapt to these effects and maintain accuracy.\n",
    "\n",
    "### 11.1 Converting pruned model to QAT and retraining\n",
    "\n",
    "To enable QAT, set the `enable_qat` parameter in the training configuration to `true`. It instructs the TAO Toolkit to simulate quantization during the training process, resulting in a model that maintains higher accuracy when actually quantized for deployment.\n",
    "\n",
    "Examine the QAT-enabled training configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the retrain experiment file. \n",
    "# Note: We have updated the experiment file to convert the\n",
    "# pretrained model to qat mode by setting the enable_qat\n",
    "# parameter.\n",
    "!cat $LOCAL_SPECS_DIR/detectnet_v2_retrain_resnet18_coco_qat.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2 Running QAT retraining\n",
    "\n",
    "Run the retraining process with QAT enabled, which will simulate the effects of quantization during training, helping the model adapt to the reduced precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao model detectnet_v2 train -e $SPECS_DIR/detectnet_v2_retrain_resnet18_coco_qat.txt \\\n",
    "                        -r $USER_EXPERIMENT_DIR/experiment_dir_retrain_qat \\\n",
    "                        -n resnet18_detector_pruned_qat \\\n",
    "                        --gpus $NUM_GPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -rlt $LOCAL_EXPERIMENT_DIR/experiment_dir_retrain_qat/weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3 Evaluating the QAT model\n",
    "\n",
    "After QAT retraining, evaluate the model's performance to ensure that the simulated quantization does not have significantly degraded accuracy. Ideally, the mAP of the QAT model should be comparable to that of the pruned and retrained model without QAT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao model detectnet_v2 evaluate -e $SPECS_DIR/detectnet_v2_retrain_resnet18_kitti_qat.txt \\\n",
    "                           -m $USER_EXPERIMENT_DIR/experiment_dir_retrain_qat/weights/resnet18_detector_pruned_qat.hdf5 \\\n",
    "                           -f tlt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.4 Exporting the QAT model to ONNX format\n",
    "\n",
    "After having a trained and evaluated QAT model, export it to ONNX format for deployment. The exported model will maintain the quantization-aware properties, making it suitable for INT8 inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf $LOCAL_EXPERIMENT_DIR/experiment_dir_final/resnet18_detector_qat.onnx\n",
    "!tao model detectnet_v2 export \\\n",
    "                  -m $USER_EXPERIMENT_DIR/experiment_dir_retrain_qat/weights/resnet18_detector_pruned_qat.hdf5 \\\n",
    "                  -o $USER_EXPERIMENT_DIR/experiment_dir_final/resnet18_detector_qat.onnx \\\n",
    "                  -e $SPECS_DIR/detectnet_v2_retrain_resnet18_coco_qat.txt \\\n",
    "                  --onnx_route tf2onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Infineon PSOC&trade; Edge MCU integration\n",
    "\n",
    "Prepare the model for deployment on Infineon PSOC&trade; Edge MCU devices by installing the required dependencies and using the Infineon tooling to convert the ONNX model to a format compatible with Infineon's neural processing units.\n",
    "\n",
    "### 12.1 Installing required dependencies\n",
    "\n",
    "Install the necessary packages for the Infineon tooling script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openvino_dev openvino2tensorflow tensorflow==2.8 tensorflow_datasets ethos-u-vela onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2 Converting the model for Infineon PSOC&trade; Edge MCU\n",
    "Use the Infineon script (`ifx_tooling.py`) to convert QAT-trained ONNX model to a format compatible with Infineon PSOC&trade; Edge MCU devices (TFLite). This process involves:\n",
    "\n",
    "- Specifying the path to the QAT ONNX model\n",
    "- Configuring the target hardware (Ethos-U55 neural processing unit)\n",
    "- Setting system parameters specific to the PSOC&trade; Edge MCU platform\n",
    "- Running the conversion process to generate deployment-ready artifacts\n",
    "\n",
    "The configuration specifically targets the Ethos-U55-128 accelerator with the PSE84_M55_U55_400MHz system configuration, which is optimized for Infineon PSOC&trade; Edge MCU devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ifx_tooling import run_ifx_tooling, ModelConversionError\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "qat_onnx_model_path = os.path.join(os.environ['LOCAL_EXPERIMENT_DIR'], \"experiment_dir_final/resnet18_detector_qat.onnx\")\n",
    "ifx_tooling_output_path = os.path.join(os.environ['LOCAL_EXPERIMENT_DIR'], \"ifx_tooling\")\n",
    "\n",
    "config = {\n",
    "    'vela_accelerator': 'ethos-u55-128',\n",
    "    'vela_system_config': 'PSE84_M55_U55_400MHz',\n",
    "    'vela_memory_mode': 'Sram_Only',\n",
    "    'compress_to_fp16': False,\n",
    "    'vela_ini_file_path': os.path.join(os.environ['LOCAL_PROJECT_DIR'], \"vela.ini\")\n",
    "}\n",
    "\n",
    "try:\n",
    "    output_paths = run_ifx_tooling(\n",
    "        onnx_model_path=qat_onnx_model_path,\n",
    "        input_shape=[1, 3, 240, 320],\n",
    "        output_dir=ifx_tooling_output_path,\n",
    "        config=config\n",
    "    )\n",
    "    print(\"Generated artifacts:\", output_paths)\n",
    "except ModelConversionError as e:\n",
    "    print(f\"Conversion failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Summary\n",
    "\n",
    "To conclude:\n",
    "\n",
    "- **Trained** on a ResNet-18 based DetectNet_v2 model on the COCO dataset\n",
    "- **Evaluated** the model to establish a performance baseline\n",
    "- **Pruned** the model to reduce its size and computational requirements\n",
    "- **Retrained** the pruned model to recover accuracy\n",
    "- **Applied quantization-aware training** to prepare the model for efficient INT8 inference\n",
    "- **Exported** the optimized model to the ONNX format\n",
    "- **Converted** the model for deployment on Infineon PSOC&trade; Edge MCU devices using the Infineon tooling  script\n",
    "\n",
    "To deploy this model on your Infineon PSOC&trade; Edge MCU device:\n",
    "\n",
    "- Transfer the generated artifacts from the `ifx_tooling` directory to your development environment\n",
    "- Use the Infineon ModusToolbox&trade; to integrate the model into your application\n",
    "- Implement the pre-processing and post-processing logic to handle inputs and outputs\n",
    "- Test and validate the deployment on your target hardware\n",
    "\n",
    "### Optimization possibilities\n",
    "\n",
    "If you need additional performance improvements:\n",
    "\n",
    "- **Data augmentation:** Enhances the training dataset with more varied examples\n",
    "- **Hyperparameter tuning:** Fine-tune learning rates, batch sizes, and other parameters\n",
    "- **Model architecture:** Considers alternative model architectures like MobileNet for more efficiency\n",
    "- **Custom dataset:** Train on a dataset specific to your application domain\n",
    "\n",
    "This workflow provides a strong foundation for developing and deploying efficient AI models on Infineon PSOC&trade; Edge MCU devices, enabling you to bring intelligence to the edge with optimized performance and resource usage."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "f23a2831654361cfd8b219e05b5055fdda3e37fe5c0b020e6226f740844c300a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
