{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NVIDIA TAO DetectNet_v2 for Infineon PSOC EDGE Devices\n",
    "\n",
    "Welcome to this comprehensive guide on training and optimizing neural networks for deployment on Infineon PSOC EDGE devices using NVIDIA's Train Adapt Optimize (TAO) Toolkit. This notebook will walk you through the complete workflow from training a custom object detection model to optimizing it for efficient deployment on resource-constrained edge devices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is TAO Toolkit?\n",
    "\n",
    "Transfer learning is the process of transferring learned features from one application to another. It is a commonly used training technique where you use a model trained on one task and re-train to use it on a different task. \n",
    "\n",
    "Train Adapt Optimize (TAO) Toolkit is a simple and easy-to-use Python based AI toolkit for taking purpose-built AI models and customizing them with users own data. The TAO Toolkit provides a streamlined workflow to:\n",
    "\n",
    "1. **Train**: Fine-tune pre-trained models with your own data\n",
    "2. **Adapt**: Adapt models to your specific use-case\n",
    "3. **Optimize**: Optimize models for efficient deployment on edge devices\n",
    "\n",
    "<img align=\"center\" src=\"https://d29g4g2dyqv443.cloudfront.net/sites/default/files/akamai/TAO/tlt-tao-toolkit-bring-your-own-model-diagram.png\" width=\"1080\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is DetectNet-V2?\n",
    "\n",
    "DetectNet_v2, also known as GridBox object detection, is a highly optimized CNN-based object detection model designed for efficient inference. It works by:\n",
    "\n",
    "1. Dividing an input image into a uniform grid\n",
    "2. Predicting four normalized bounding-box parameters (xc, yc, w, h) and confidence value per output class for each grid cell\n",
    "3. Post-processing detections using clustering algorithms such as DBSCAN, NMS, or HYBRID (DBSCAN + NMS)\n",
    "\n",
    "DetectNet_v2 is particularly well-suited for resource-constrained environments like the Infineon PSOC EDGE devices due to its computational efficiency while maintaining high accuracy.\n",
    "\n",
    "### Sample output predictions from a trained DetectNet_v2 model\n",
    "\n",
    "<img align=\"center\" src=\"https://miro.medium.com/v2/resize:fit:720/0*YaQDIKR4gRbP2-by\" width=\"960\">\n",
    "\n",
    "<img align=\"center\" src=\"https://miro.medium.com/v2/resize:fit:720/0*eY1qluSyldYl9qDw\" width=\"960\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow Overview and Learning Objectives\n",
    "\n",
    "This notebook demonstrates how to leverage NVIDIA TAO to train and optimize a DetectNet_v2 model for deployment on Infineon PSOC EDGE devices. You will learn how to:\n",
    "\n",
    "1. **Environment Setup**: Configure your environment for TAO toolkit\n",
    "2. **Data Preparation**: Prepare your dataset in the required format for training\n",
    "3. **Model Training**: Train a ResNet-18 based DetectNet_v2 model on the COCO dataset\n",
    "4. **Model Evaluation**: Evaluate the trained model's performance\n",
    "5. **Model Pruning**: Optimize the model size by pruning unnecessary weights\n",
    "6. **Model Retraining**: Recover accuracy after pruning through retraining\n",
    "7. **Quantization-Aware Training (QAT)**: Further optimize the model for deployment on edge devices\n",
    "8. **Model Export**: Export the optimized model to ONNX format for deployment\n",
    "9. **Infineon Toolchain Integration**: Convert the model for Infineon PSOC EDGE devices\n",
    "\n",
    "By the end of this notebook, you will have a trained, pruned, quantized, and deployment-ready object detection model optimized for Infineon PSOC EDGE devices."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "Let's begin by setting up our environment variables and configuring the workspace. These variables will be used throughout the notebook to reference directories and files.\n",
    "\n",
    "### 1.1 Setting Environment Variables\n",
    "\n",
    "The following cell sets up environment variables that define:\n",
    "- The number of GPUs to use for training\n",
    "- Directory paths for experiments, data, and specifications\n",
    "\n",
    "**Note**: Please make sure to remove any stray artifacts/files from previous experiments in `$USER_EXPERIMENT_DIR` or `$DATA_DOWNLOAD_DIR` as they may interfere with creating a training graph for a new experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up env variables for cleaner command line commands.\n",
    "import os\n",
    "\n",
    "%env NUM_GPUS=2\n",
    "%env USER_EXPERIMENT_DIR=/workspace/tao-experiments/detectnet_v2\n",
    "%env DATA_DOWNLOAD_DIR=/workspace/tao-experiments/data\n",
    "\n",
    "# Set this path if you don't run the notebook from the samples directory.\n",
    "# %env NOTEBOOK_ROOT=~/tao-samples/detectnet_v2\n",
    "\n",
    "# Please define this local project directory that needs to be mapped to the TAO docker session.\n",
    "# The dataset expected to be present in $LOCAL_PROJECT_DIR/data, while the results for the steps\n",
    "# in this notebook will be stored at $LOCAL_PROJECT_DIR/detectnet_v2\n",
    "# !PLEASE MAKE SURE TO UPDATE THIS PATH!.\n",
    "\n",
    "os.environ[\"LOCAL_PROJECT_DIR\"] = \"/teamspace/studios/this_studio\"\n",
    "\n",
    "os.environ[\"LOCAL_DATA_DIR\"] = os.path.join(\n",
    "    os.getenv(\"LOCAL_PROJECT_DIR\", os.getcwd()),\n",
    "    \"data\"\n",
    ")\n",
    "os.environ[\"LOCAL_EXPERIMENT_DIR\"] = os.path.join(\n",
    "    os.getenv(\"LOCAL_PROJECT_DIR\", os.getcwd()),\n",
    "    \"detectnet_v2\"\n",
    ")\n",
    "\n",
    "# Make the experiment directory \n",
    "! mkdir -p $LOCAL_EXPERIMENT_DIR\n",
    "\n",
    "# The sample spec files are present in the same path as the downloaded samples.\n",
    "os.environ[\"LOCAL_SPECS_DIR\"] = os.path.join(\n",
    "    os.getenv(\"NOTEBOOK_ROOT\", os.getcwd()),\n",
    "    \"specs\"\n",
    ")\n",
    "%env SPECS_DIR=/workspace/tao-experiments/detectnet_v2/specs\n",
    "CLEARML_LOGGED_IN = False\n",
    "WANDB_LOGGED_IN = False\n",
    "\n",
    "# Showing list of specification files.\n",
    "!ls -rlt $LOCAL_SPECS_DIR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Mapping Directories to TAO Docker\n",
    "\n",
    "The following cell maps our local directories to the TAO docker container. This allows TAO to access our data and save the results properly.\n",
    "\n",
    "The mapping includes:\n",
    "- The data directory containing our dataset\n",
    "- The specs directory containing configuration files\n",
    "- The experiment directory where outputs will be stored\n",
    "\n",
    "This step is crucial for ensuring proper data access and result persistence between your local filesystem and the TAO docker environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping up the local directories to the TAO docker.\n",
    "import json\n",
    "mounts_file = os.path.expanduser(\"~/.tao_mounts.json\")\n",
    "\n",
    "# Define the dictionary with the mapped drives\n",
    "drive_map = {\n",
    "    \"Mounts\": [\n",
    "        # Mapping the data directory\n",
    "        {\n",
    "            \"source\": os.environ[\"LOCAL_PROJECT_DIR\"],\n",
    "            \"destination\": \"/workspace/tao-experiments\"\n",
    "        },\n",
    "        # Mapping the specs directory.\n",
    "        {\n",
    "            \"source\": os.environ[\"LOCAL_SPECS_DIR\"],\n",
    "            \"destination\": os.environ[\"SPECS_DIR\"]\n",
    "        },\n",
    "        {\n",
    "            \"source\": \"/teamspace/studios/this_studio/data\",\n",
    "            \"destination\": \"/workspace/tao-experiments/data\"\n",
    "        }\n",
    "    ],\n",
    "    \"DockerOptions\":{\n",
    "        \"user\": f\"{os.getuid()}:{os.getgid()}\"\n",
    "    }\n",
    "}\n",
    "\n",
    "if CLEARML_LOGGED_IN:\n",
    "    if \"Envs\" not in drive_map.keys():\n",
    "        drive_map[\"Envs\"] = []\n",
    "    drive_map[\"Envs\"].extend([\n",
    "        {\n",
    "            \"variable\": \"CLEARML_WEB_HOST\",\n",
    "            \"value\": os.getenv(\"CLEARML_WEB_HOST\")\n",
    "        },\n",
    "        {\n",
    "            \"variable\": \"CLEARML_API_HOST\",\n",
    "            \"value\": os.getenv(\"CLEARML_API_HOST\")\n",
    "        },\n",
    "        {\n",
    "            \"variable\": \"CLEARML_FILES_HOST\",\n",
    "            \"value\": os.getenv(\"CLEARML_FILES_HOST\")\n",
    "        },\n",
    "        {\n",
    "            \"variable\": \"CLEARML_API_ACCESS_KEY\",\n",
    "            \"value\": os.getenv(\"CLEARML_API_ACCESS_KEY\")\n",
    "        },\n",
    "        {\n",
    "            \"variable\": \"CLEARML_API_SECRET_KEY\",\n",
    "            \"value\": os.getenv(\"CLEARML_API_SECRET_KEY\")\n",
    "        },\n",
    "    ])\n",
    "\n",
    "if WANDB_LOGGED_IN:\n",
    "    if \"Envs\" not in drive_map.keys():\n",
    "        drive_map[\"Envs\"] = []\n",
    "    # Weights and biases currently requires access to the\n",
    "    # /.config directory in the docker. Therefore, the docker\n",
    "    # must be instantiated as root user. With the cells mentioned below\n",
    "    # we will be deleting the cells that set user roles.\n",
    "    if \"user\" in drive_map[\"DockerOptions\"].keys():\n",
    "        del(drive_map[\"DockerOptions\"][\"user\"])\n",
    "    drive_map[\"Envs\"].extend([\n",
    "        {\n",
    "            \"variable\": \"WANDB_API_KEY\",\n",
    "            \"value\": os.getenv(\"WANDB_API_KEY\")\n",
    "        }\n",
    "    ])\n",
    "\n",
    "# Writing the mounts file.\n",
    "with open(mounts_file, \"w\") as mfile:\n",
    "    json.dump(drive_map, mfile, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!cat ~/.tao_mounts.json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Installing the TAO Launcher\n",
    "\n",
    "The TAO launcher is a Python package that provides the interface to the TAO toolkit. It's distributed as a Python wheel on PyPI and can be installed with pip.\n",
    "\n",
    "**Prerequisites**:\n",
    "- Python >=3.7, <=3.10.x\n",
    "- docker-ce > 19.03.5\n",
    "- docker-API 1.40\n",
    "- nvidia-container-toolkit > 1.3.0-1\n",
    "- nvidia-container-runtime > 3.4.0-1\n",
    "- nvidia-docker2 > 2.5.0-1\n",
    "- nvidia-driver > 455+\n",
    "\n",
    "After installation, we verify the TAO launcher version to ensure everything is set up correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKIP this step IF you have already installed the TAO launcher wheel.\n",
    "!pip3 install nvidia-tao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the versions of the TAO launcher\n",
    "!tao info --verbose"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare dataset and pre-trained model <a class=\"anchor\" id=\"head-2\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "\n",
    "In this section, we'll prepare the COCO dataset for training our object detection model. The dataset must be properly structured and converted to TFRecords format for efficient training.\n",
    "\n",
    "### 2.1 Downloading the Dataset\n",
    "\n",
    "We'll download the COCO dataset, which contains images and annotations for common objects in context. This dataset is widely used for object detection tasks and provides a good baseline for training models that can later be fine-tuned for specific applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create local dir\n",
    "!mkdir -p $LOCAL_DATA_DIR\n",
    "!mkdir -p $LOCAL_EXPERIMENT_DIR\n",
    "# Download and preprocess data\n",
    "!tao model detectnet_v2 run bash $SPECS_DIR/download_coco.sh $DATA_DOWNLOAD_DIR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Verifying the Downloaded Dataset\n",
    "\n",
    "Let's verify that our dataset has been correctly downloaded by checking the number of training images, labels, and testing images. This step ensures we have all the necessary data before proceeding to model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify\n",
    "import os\n",
    "\n",
    "DATA_DIR = os.environ.get('LOCAL_DATA_DIR')\n",
    "num_training_images = len(os.listdir(os.path.join(DATA_DIR, \"raw-data/train2017\")))\n",
    "num_val_images = len(os.listdir(os.path.join(DATA_DIR, \"raw-data/val2017\")))\n",
    "print(\"Number of images in the train set. {}\".format(num_training_images))\n",
    "print(\"Number of images in the val set. {}\".format(num_val_images))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Converting Dataset to TFRecords\n",
    "\n",
    "For efficient training with NVIDIA TAO, we need to convert our dataset to TFRecords format. TFRecords is the native file format for TensorFlow, optimized for handling large datasets and enabling faster data loading during training.\n",
    "\n",
    "The following steps involve:\n",
    "1. Reviewing the TFRecords conversion specification file\n",
    "2. Creating a directory for storing the TFRecords\n",
    "3. Running the conversion process\n",
    "4. Verifying the generated TFRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TFrecords conversion spec file for kitti training\")\n",
    "!cat $LOCAL_SPECS_DIR/detectnet_v2_tfrecords_coco_trainval.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new directory for the output tfrecords dump.\n",
    "print(\"Converting Tfrecords for kitti trainval dataset\")\n",
    "!mkdir -p $LOCAL_DATA_DIR/tfrecords\n",
    "!tao model detectnet_v2 dataset_convert \\\n",
    "                  -d $SPECS_DIR/detectnet_v2_tfrecords_coco_trainval.txt \\\n",
    "                  -o $DATA_DOWNLOAD_DIR/tfrecords/coco_trainval/coco_trainval \\\n",
    "                  -r $USER_EXPERIMENT_DIR/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -rlt $LOCAL_DATA_DIR/tfrecords/coco_trainval/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao model detectnet_v2 run bash $SPECS_DIR/check_data.sh"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Downloading the Pre-trained Model\n",
    "\n",
    "To leverage transfer learning, we'll download a pre-trained ResNet-18 model from NVIDIA's NGC registry. Using a pre-trained model significantly reduces training time and often leads to better performance, especially when training data is limited.\n",
    "\n",
    "For DetectNet_v2, the input is expected to be 0-1 normalized with input channels in RGB order. Therefore, we use models from the `nvidia/tao/pretrained_detectnet_v2` repository which are specifically prepared for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing NGC CLI on the local machine.\n",
    "## Download and install\n",
    "%env CLI=ngccli_cat_linux.zip\n",
    "!mkdir -p $LOCAL_PROJECT_DIR/ngccli\n",
    "\n",
    "# Remove any previously existing CLI installations\n",
    "!rm -rf $LOCAL_PROJECT_DIR/ngccli/*\n",
    "!wget \"https://ngc.nvidia.com/downloads/$CLI\" -P $LOCAL_PROJECT_DIR/ngccli\n",
    "!unzip -u \"$LOCAL_PROJECT_DIR/ngccli/$CLI\" -d $LOCAL_PROJECT_DIR/ngccli/\n",
    "!rm $LOCAL_PROJECT_DIR/ngccli/*.zip \n",
    "os.environ[\"PATH\"]=\"{}/ngccli/ngc-cli:{}\".format(os.getenv(\"LOCAL_PROJECT_DIR\", \"\"), os.getenv(\"PATH\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List models available in the model registry.\n",
    "!ngc registry model list nvidia/tao/pretrained_detectnet_v2:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the target destination to download the model.\n",
    "!mkdir -p $LOCAL_EXPERIMENT_DIR/pretrained_resnet18/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the pretrained model from NGC\n",
    "!ngc registry model download-version nvidia/tao/pretrained_detectnet_v2:resnet18 \\\n",
    "    --dest $LOCAL_EXPERIMENT_DIR/pretrained_resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -rlt $LOCAL_EXPERIMENT_DIR/pretrained_resnet18/pretrained_detectnet_v2_vresnet18"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Configuration\n",
    "\n",
    "Now that we have our dataset and pre-trained model, we need to configure the training parameters. The training specification file defines various aspects of the training process, including:\n",
    "\n",
    "- Dataset paths and configurations\n",
    "- Pre-trained model path\n",
    "- Augmentation parameters for data diversification\n",
    "- Training hyperparameters (batch size, learning rate, epochs, etc.)\n",
    "- Evaluation settings\n",
    "\n",
    "Let's examine the training specification file to understand the configuration for our DetectNet_v2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat $LOCAL_SPECS_DIR/detectnet_v2_train_resnet18_coco.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training\n",
    "\n",
    "With our environment set up, data prepared, and training configuration defined, we can now train our DetectNet_v2 model. This process will fine-tune the pre-trained ResNet-18 model on our specific dataset.\n",
    "\n",
    "**Important Notes**:\n",
    "- Training may take several hours to complete depending on your GPU configuration\n",
    "- DetectNet_v2 supports restart from checkpoint if training is interrupted\n",
    "- When using multiple GPUs, you may need to adjust batch size and learning rate accordingly\n",
    "\n",
    "First, we'll ensure the experiment directory is clean by removing any previous training artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf $USER_EXPERIMENT_DIR/experiment_dir_unpruned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Running the Training Process\n",
    "\n",
    "Now we'll start the training process using the TAO toolkit. The command below:\n",
    "1. Specifies the model type (detectnet_v2)\n",
    "2. Provides the training specification file\n",
    "3. Sets the output directory for the trained model\n",
    "4. Names the model for easier reference\n",
    "5. Specifies the number of GPUs to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao model detectnet_v2 train -e $SPECS_DIR/detectnet_v2_train_resnet18_coco.txt \\\n",
    "                        -r $USER_EXPERIMENT_DIR/experiment_dir_unpruned \\\n",
    "                        -n resnet18_detector \\\n",
    "                        --gpus $NUM_GPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model for each epoch:')\n",
    "print('---------------------')\n",
    "!ls -lh $LOCAL_EXPERIMENT_DIR/experiment_dir_unpruned/weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation\n",
    "\n",
    "After training our model, we need to evaluate its performance to understand how well it performs on the validation dataset. This gives us a baseline metric before we proceed with optimization steps.\n",
    "\n",
    "The evaluation process calculates key metrics like precision, recall, and mean Average Precision (mAP) to quantify the model's accuracy in detecting objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao model detectnet_v2 evaluate -e $SPECS_DIR/detectnet_v2_train_resnet18_coco.txt \\\n",
    "                           -m $USER_EXPERIMENT_DIR/experiment_dir_unpruned/weights/resnet18_detector.hdf5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Pruning\n",
    "\n",
    "Pruning is a model optimization technique that removes unnecessary weights from the neural network, resulting in a smaller model with faster inference times. This is particularly important for deployment on resource-constrained devices like Infineon PSOC EDGE.\n",
    "\n",
    "### 6.1 Understanding Pruning Parameters\n",
    "\n",
    "The key parameters for pruning are:\n",
    "\n",
    "- **Pruning threshold (`-pth`)**: Controls the trade-off between model size and accuracy. Higher values result in smaller models but potentially lower accuracy.\n",
    "- **Equalization layers (`-el`)**: Specifies which layers should be considered for pruning.\n",
    "\n",
    "**Note**: The optimal pruning threshold depends on your specific dataset and requirements. A value of 0.01 is often a good starting point for DetectNet_v2 models, but you may need to experiment to find the best balance between model size and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an output directory if it doesn't exist.\n",
    "!mkdir -p $LOCAL_EXPERIMENT_DIR/experiment_dir_pruned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Performing Initial Pruning\n",
    "\n",
    "We'll perform an initial pruning step targeting specific layers of the ResNet-18 model with a pruning threshold of 0.9. This removes a significant portion of the model weights while preserving the most important ones for maintaining accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao model detectnet_v2 prune \\\n",
    "                  -m $USER_EXPERIMENT_DIR/experiment_dir_unpruned/weights/resnet18_detector.hdf5 \\\n",
    "                  -o $USER_EXPERIMENT_DIR/experiment_dir_pruned/resnet18_nopool_bn_detectnet_v2_pruned.hdf5 \\\n",
    "                  -pth 0.9 \\\n",
    "                  -el block_1b_conv_1 block_1b_conv_2 \\\n",
    "                    block_1b_conv_shortcut block_1c_conv_1 block_1c_conv_2 \\\n",
    "                    block_1c_conv_shortcut block_2a_conv_1 block_2a_conv_2 \\\n",
    "                    block_2a_conv_shortcut block_2b_conv_1 block_2b_conv_2 \\\n",
    "                    block_2b_conv_shortcut block_2c_conv_1 block_2c_conv_2 \\\n",
    "                    block_2c_conv_shortcut block_2d_conv_1 block_2d_conv_2 \\\n",
    "                    block_2d_conv_shortcut block_3a_conv_1 block_3a_conv_2 \\\n",
    "                    block_3a_conv_shortcut block_3b_conv_1 block_3b_conv_2 \\\n",
    "                    block_3b_conv_shortcut block_3c_conv_1 block_3c_conv_2 \\\n",
    "                    block_3c_conv_shortcut block_3d_conv_1 block_3d_conv_2 \\\n",
    "                    block_3d_conv_shortcut block_3e_conv_1 block_3e_conv_2 \\\n",
    "                    block_3e_conv_shortcut block_3f_conv_1 block_3f_conv_2 \\\n",
    "                    block_3f_conv_shortcut  block_4a_conv_1 block_4b_conv_1 \\\n",
    "                    block_4c_conv_1 block_4a_conv_2 block_4b_conv_2 \\\n",
    "                    block_4c_conv_2 block_4a_conv_shortcut \\\n",
    "                    block_4b_conv_shortcut block_4c_conv_shortcut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -rlt $LOCAL_EXPERIMENT_DIR/experiment_dir_pruned/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Performing Additional Pruning\n",
    "\n",
    "To further reduce the model size, we'll apply a second pruning step to the model, targeting additional layers with a pruning threshold of 0.8. This progressive pruning approach helps maintain a better balance between model size and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao model detectnet_v2 prune \\\n",
    "                  -m $USER_EXPERIMENT_DIR/experiment_dir_pruned/resnet18_nopool_bn_detectnet_v2_pruned.hdf5 \\\n",
    "                  -o $USER_EXPERIMENT_DIR/experiment_dir_pruned/resnet18_nopool_bn_detectnet_v2_pruned_final.hdf5 \\\n",
    "                  -pth 0.8 \\\n",
    "                  -el -el conv1 block_1a_conv_1 block_1a_conv_2 block_1a_conv_shortcut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -rlt $LOCAL_EXPERIMENT_DIR/experiment_dir_pruned/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Retraining\n",
    "\n",
    "After pruning, the model typically experiences some accuracy degradation. To recover this lost accuracy, we need to retrain the pruned model. This retraining process preserves the smaller model size while improving its performance.\n",
    "\n",
    "### 7.1 Configuring Retraining\n",
    "\n",
    "For retraining, we need to:\n",
    "1. Use the pruned model as the starting point\n",
    "2. Set `load_graph` to `true` in the model configuration to load the pruned model's architecture\n",
    "3. Adjust training parameters if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the retrain experiment file. \n",
    "# Note: We have updated the experiment file to include the \n",
    "# newly pruned model as a pretrained weights and, the\n",
    "# load_graph option is set to true \n",
    "!cat $LOCAL_SPECS_DIR/detectnet_v2_retrain_resnet18_coco.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Running Retraining\n",
    "\n",
    "Now we'll retrain the pruned model to recover accuracy. This process typically requires fewer epochs than the initial training since we're fine-tuning rather than training from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retraining using the pruned model as pretrained weights \n",
    "!tao model detectnet_v2 train -e $SPECS_DIR/detectnet_v2_retrain_resnet18_coco.txt \\\n",
    "                        -r $USER_EXPERIMENT_DIR/experiment_dir_retrain \\\n",
    "                        -n resnet18_detector_pruned \\\n",
    "                        --gpus $NUM_GPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing the newly retrained model.\n",
    "!ls -rlt $LOCAL_EXPERIMENT_DIR/experiment_dir_retrain/weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluating the Retrained Model\n",
    "\n",
    "After retraining, we need to evaluate the pruned and retrained model to ensure it has recovered the accuracy lost during pruning. This step helps us validate that our optimized model maintains acceptable performance for our use case."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section evaluates the pruned and retrained model, using the `evaluate` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao model detectnet_v2 evaluate -e $SPECS_DIR/detectnet_v2_retrain_resnet18_coco.txt \\\n",
    "                           -m $USER_EXPERIMENT_DIR/experiment_dir_retrain/weights/resnet18_detector_pruned.hdf5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualizing Inferences\n",
    "\n",
    "To get a qualitative understanding of our model's performance, we'll run inference on test images and visualize the detection results. This helps us identify any specific weaknesses or strengths in our model's detection capabilities.\n",
    "\n",
    "### 9.1 Preparing Test Samples\n",
    "\n",
    "First, we'll select a subset of images from our validation set to use for visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $LOCAL_DATA_DIR/test_samples\n",
    "!cp $(ls $LOCAL_DATA_DIR/raw-data/val2017/* | head -n 20) $LOCAL_DATA_DIR/test_samples/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Running Inference\n",
    "\n",
    "Now we'll run inference on our test samples using the trained model. The inference process will:\n",
    "1. Process each image in the test directory\n",
    "2. Generate bounding box predictions using our trained DetectNet_v2 model\n",
    "3. Save annotated images and detection labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running inference for detection on n images\n",
    "!tao model detectnet_v2 inference -e $SPECS_DIR/detectnet_v2_inference_coco.txt \\\n",
    "                            -r $USER_EXPERIMENT_DIR/tlt_infer_testing \\\n",
    "                            -i $DATA_DOWNLOAD_DIR/test_samples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `inference` tool produces two outputs. \n",
    "1. Overlain images in `$USER_EXPERIMENT_DIR/tlt_infer_testing/images_annotated`\n",
    "2. Frame by frame bbox labels in kitti format located in `$USER_EXPERIMENT_DIR/tlt_infer_testing/labels`\n",
    "\n",
    "*Note: To run inferences for a single image, simply replace the path to the -i flag in `inference` command with the path to the image.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Visualizing Results\n",
    "\n",
    "The inference process produces two types of outputs:\n",
    "1. Annotated images with bounding boxes in `$USER_EXPERIMENT_DIR/tlt_infer_testing/images_annotated`\n",
    "2. Detection labels in KITTI format in `$USER_EXPERIMENT_DIR/tlt_infer_testing/labels`\n",
    "\n",
    "Let's create a function to visualize these results in a grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple grid visualizer\n",
    "!pip3 install \"matplotlib>=3.3.3, <4.0\"\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from math import ceil\n",
    "valid_image_ext = ['.jpg', '.png', '.jpeg', '.ppm']\n",
    "\n",
    "def visualize_images(image_dir, num_cols=4, num_images=10):\n",
    "    output_path = os.path.join(os.environ['LOCAL_EXPERIMENT_DIR'], image_dir)\n",
    "    num_rows = int(ceil(float(num_images) / float(num_cols)))\n",
    "    f, axarr = plt.subplots(num_rows, num_cols, figsize=[80,30])\n",
    "    f.tight_layout()\n",
    "    a = [os.path.join(output_path, image) for image in os.listdir(output_path) \n",
    "         if os.path.splitext(image)[1].lower() in valid_image_ext]\n",
    "    for idx, img_path in enumerate(a[:num_images]):\n",
    "        col_id = idx % num_cols\n",
    "        row_id = idx // num_cols\n",
    "        img = plt.imread(img_path)\n",
    "        axarr[row_id, col_id].imshow(img) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's display the first 12 annotated images to visually inspect our model's detection performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the first 12 images.\n",
    "OUTPUT_PATH = 'tlt_infer_testing/images_annotated' # relative path from $USER_EXPERIMENT_DIR.\n",
    "COLS = 4 # number of columns in the visualizer grid.\n",
    "IMAGES = 12 # number of images to visualize.\n",
    "\n",
    "visualize_images(OUTPUT_PATH, num_cols=COLS, num_images=IMAGES)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Export\n",
    "\n",
    "For deployment on Infineon PSOC EDGE devices, we need to export our trained and optimized model to ONNX format. ONNX (Open Neural Network Exchange) is an open format for representing deep learning models that enables interoperability between different frameworks.\n",
    "\n",
    "In this step, we'll export our pruned and retrained model to ONNX format using the TensorFlow-to-ONNX converter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $LOCAL_EXPERIMENT_DIR/experiment_dir_final\n",
    "# Removing a pre-existing copy of the onnx if there has been any.\n",
    "import os\n",
    "output_file=os.path.join(os.environ['LOCAL_EXPERIMENT_DIR'],\n",
    "                         \"experiment_dir_final/resnet18_detector.onnx\")\n",
    "if os.path.exists(output_file):\n",
    "    os.system(\"rm {}\".format(output_file))\n",
    "!tao model detectnet_v2 export \\\n",
    "                  -m $USER_EXPERIMENT_DIR/experiment_dir_retrain/weights/resnet18_detector_pruned.hdf5 \\\n",
    "                  -e $SPECS_DIR/detectnet_v2_retrain_resnet18_coco.txt \\\n",
    "                  -o $USER_EXPERIMENT_DIR/experiment_dir_final/resnet18_detector.onnx \\\n",
    "                  --onnx_route tf2onnx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Quantization-Aware Training (QAT)\n",
    "\n",
    "Quantization is a technique to further reduce model size and improve inference speed by representing weights and activations with lower precision (e.g., INT8 instead of FP32). This is especially important for deployment on resource-constrained edge devices like Infineon PSOC EDGE.\n",
    "\n",
    "While post-training quantization is simple, it often leads to accuracy degradation. Quantization-Aware Training (QAT) addresses this by simulating quantization effects during training, allowing the model to adapt to these effects and maintain accuracy.\n",
    "\n",
    "### 11.1 Converting Pruned Model to QAT and Retraining\n",
    "\n",
    "To enable QAT, we simply set the `enable_qat` parameter in the training configuration to `true`. This instructs the TAO toolkit to simulate quantization during the training process, resulting in a model that maintains higher accuracy when actually quantized for deployment.\n",
    "\n",
    "Let's first examine our QAT-enabled training configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the retrain experiment file. \n",
    "# Note: We have updated the experiment file to convert the\n",
    "# pretrained model to qat mode by setting the enable_qat\n",
    "# parameter.\n",
    "!cat $LOCAL_SPECS_DIR/detectnet_v2_retrain_resnet18_coco_qat.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2 Running QAT Retraining\n",
    "\n",
    "Next, we'll run the retraining process with QAT enabled. This will simulate the effects of quantization during training, helping the model adapt to the reduced precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao model detectnet_v2 train -e $SPECS_DIR/detectnet_v2_retrain_resnet18_coco_qat.txt \\\n",
    "                        -r $USER_EXPERIMENT_DIR/experiment_dir_retrain_qat \\\n",
    "                        -n resnet18_detector_pruned_qat \\\n",
    "                        --gpus $NUM_GPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -rlt $LOCAL_EXPERIMENT_DIR/experiment_dir_retrain_qat/weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3 Evaluating the QAT Model\n",
    "\n",
    "After QAT retraining, we need to evaluate the model's performance to ensure that the simulated quantization hasn't significantly degraded accuracy. Ideally, the mAP of the QAT model should be comparable to that of the pruned and retrained model without QAT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao model detectnet_v2 evaluate -e $SPECS_DIR/detectnet_v2_retrain_resnet18_kitti_qat.txt \\\n",
    "                           -m $USER_EXPERIMENT_DIR/experiment_dir_retrain_qat/weights/resnet18_detector_pruned_qat.hdf5 \\\n",
    "                           -f tlt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.4 Exporting the QAT Model to ONNX\n",
    "\n",
    "Now that we have a trained and evaluated QAT model, we need to export it to ONNX format for deployment. The exported model will maintain the quantization-aware properties, making it suitable for INT8 inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf $LOCAL_EXPERIMENT_DIR/experiment_dir_final/resnet18_detector_qat.onnx\n",
    "!tao model detectnet_v2 export \\\n",
    "                  -m $USER_EXPERIMENT_DIR/experiment_dir_retrain_qat/weights/resnet18_detector_pruned_qat.hdf5 \\\n",
    "                  -o $USER_EXPERIMENT_DIR/experiment_dir_final/resnet18_detector_qat.onnx \\\n",
    "                  -e $SPECS_DIR/detectnet_v2_retrain_resnet18_coco_qat.txt \\\n",
    "                  --onnx_route tf2onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Infineon PSOC EDGE Integration\n",
    "\n",
    "In this final section, we'll prepare our model for deployment on Infineon PSOC EDGE devices using the Infineon Toolchain. This involves installing the necessary dependencies and using the IFX Tooling to convert our ONNX model to a format compatible with Infineon's neural processing units.\n",
    "\n",
    "### 12.1 Installing Required Dependencies\n",
    "\n",
    "First, we need to install the necessary packages for the Infineon toolchain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openvino_dev openvino2tensorflow tensorflow==2.8 tensorflow_datasets ethos-u-vela onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2 Converting the Model for Infineon PSOC EDGE\n",
    "\n",
    "Now we'll use the Infineon Toolchain (IFX Tooling) to convert our QAT-trained ONNX model to a format compatible with Infineon PSOC EDGE devices. This process involves:\n",
    "\n",
    "1. Specifying the path to our QAT ONNX model\n",
    "2. Configuring the target hardware (Ethos-U55 neural processing unit)\n",
    "3. Setting system parameters specific to the PSOC EDGE platform\n",
    "4. Running the conversion process to generate deployment-ready artifacts\n",
    "\n",
    "The configuration specifically targets the Ethos-U55-128 accelerator with the PSE84_M55_U55_400MHz system configuration, which is optimized for Infineon PSOC EDGE devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ifx_tooling import run_ifx_tooling, ModelConversionError\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "qat_onnx_model_path = os.path.join(os.environ['LOCAL_EXPERIMENT_DIR'], \"experiment_dir_final/resnet18_detector_qat.onnx\")\n",
    "ifx_tooling_output_path = os.path.join(os.environ['LOCAL_EXPERIMENT_DIR'], \"ifx_tooling\")\n",
    "\n",
    "config = {\n",
    "    'vela_accelerator': 'ethos-u55-128',\n",
    "    'vela_system_config': 'PSE84_M55_U55_400MHz',\n",
    "    'vela_memory_mode': 'Sram_Only',\n",
    "    'compress_to_fp16': False,\n",
    "    'vela_ini_file_path': os.path.join(os.environ['LOCAL_PROJECT_DIR'], \"vela.ini\")\n",
    "}\n",
    "\n",
    "try:\n",
    "    output_paths = run_ifx_tooling(\n",
    "        onnx_model_path=qat_onnx_model_path,\n",
    "        input_shape=[1, 3, 240, 320],\n",
    "        output_dir=ifx_tooling_output_path,\n",
    "        config=config\n",
    "    )\n",
    "    print(\"Generated artifacts:\", output_paths)\n",
    "except ModelConversionError as e:\n",
    "    print(f\"Conversion failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Conclusion and Next Steps\n",
    "\n",
    "Congratulations! You have successfully:\n",
    "\n",
    "1. **Trained** a ResNet-18 based DetectNet_v2 model on the COCO dataset\n",
    "2. **Evaluated** the model to establish a performance baseline\n",
    "3. **Pruned** the model to reduce its size and computational requirements\n",
    "4. **Retrained** the pruned model to recover accuracy\n",
    "5. **Applied Quantization-Aware Training** to prepare the model for efficient INT8 inference\n",
    "6. **Exported** the optimized model to ONNX format\n",
    "7. **Converted** the model for deployment on Infineon PSOC EDGE devices using the Infineon Toolchain\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "To deploy this model on your Infineon PSOC EDGE device:\n",
    "\n",
    "1. Transfer the generated artifacts from the `ifx_tooling` directory to your development environment\n",
    "2. Use the Infineon ModusToolbox™ to integrate the model into your application\n",
    "3. Implement the pre-processing and post-processing logic to handle inputs and outputs\n",
    "4. Test and validate the deployment on your target hardware\n",
    "\n",
    "### Further Optimization Possibilities\n",
    "\n",
    "If you need additional performance improvements:\n",
    "\n",
    "1. **Data Augmentation**: Enhance the training dataset with more varied examples\n",
    "2. **Hyperparameter Tuning**: Fine-tune learning rates, batch sizes, and other parameters\n",
    "3. **Model Architecture**: Consider alternative model architectures like MobileNet for even more efficiency\n",
    "4. **Custom Dataset**: Train on a dataset specific to your application domain\n",
    "\n",
    "This workflow provides a strong foundation for developing and deploying efficient AI models on Infineon PSOC EDGE devices, enabling you to bring intelligence to the edge with optimized performance and resource usage."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "f23a2831654361cfd8b219e05b5055fdda3e37fe5c0b020e6226f740844c300a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
