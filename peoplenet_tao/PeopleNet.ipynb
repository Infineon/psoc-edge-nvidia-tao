{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PeopleNet Deployment for Infineon PSOC EDGE Devices\n",
    "\n",
    "This notebook demonstrates how to deploy NVIDIA's pre-trained PeopleNet model to Infineon PSOC EDGE devices. PeopleNet is a highly optimized deep learning model for detecting and tracking people in images and video streams, making it ideal for edge applications like occupancy monitoring, crowd analysis, and security systems.\n",
    "\n",
    "By following this workflow, you'll learn how to obtain a quantized PeopleNet model and convert it for efficient execution on Infineon's PSOC EDGE hardware with Arm Ethos-U55 NPU acceleration.\n",
    "\n",
    "**Note:** This notebook requires an NVIDIA GPU for the conversion process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "First, we'll set up the environment variables needed for our workflow. These variables define paths for data, experiments, and specify GPU resources.\n",
    "\n",
    "The following cell configures:\n",
    "- Number of GPUs to use (1 is sufficient for deployment)\n",
    "- Working directories for the TAO environment\n",
    "- Local directories for data storage and experiment outputs\n",
    "\n",
    "**Important:** Make sure to update the `LOCAL_PROJECT_DIR` path to match your system configuration if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up env variables for cleaner command line commands.\n",
    "import os\n",
    "\n",
    "%env NUM_GPUS=1\n",
    "%env USER_EXPERIMENT_DIR=/workspace/tao-experiments/peoplenet_onnx\n",
    "%env DATA_DOWNLOAD_DIR=/workspace/tao-experiments/data\n",
    "\n",
    "# Set this path if you don't run the notebook from the samples directory.\n",
    "# %env NOTEBOOK_ROOT=~/tao-samples/detectnet_v2\n",
    "\n",
    "# Please define this local project directory that needs to be mapped to the TAO docker session.\n",
    "# The dataset expected to be present in $LOCAL_PROJECT_DIR/data, while the results for the steps\n",
    "# in this notebook will be stored at $LOCAL_PROJECT_DIR/detectnet_v2\n",
    "# !PLEASE MAKE SURE TO UPDATE THIS PATH!.\n",
    "\n",
    "os.environ[\"LOCAL_PROJECT_DIR\"] = FIXME\n",
    "\n",
    "os.environ[\"LOCAL_DATA_DIR\"] = os.path.join(\n",
    "    os.getenv(\"LOCAL_PROJECT_DIR\", os.getcwd()),\n",
    "    \"data\"\n",
    ")\n",
    "os.environ[\"LOCAL_EXPERIMENT_DIR\"] = os.path.join(\n",
    "    os.getenv(\"LOCAL_PROJECT_DIR\", os.getcwd()),\n",
    "    \"peoplenet\"\n",
    ")\n",
    "\n",
    "# Make the experiment directory \n",
    "! mkdir -p $LOCAL_EXPERIMENT_DIR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Installing NGC Command Line Interface\n",
    "\n",
    "To download the pre-trained PeopleNet model, we need NVIDIA's NGC CLI (Command Line Interface) tool. NGC (NVIDIA GPU Cloud) hosts optimized deep learning models, frameworks, and software.\n",
    "\n",
    "The following cell:\n",
    "1. Sets up the environment for NGC CLI installation\n",
    "2. Downloads the NGC CLI package\n",
    "3. Extracts and configures the tool for use\n",
    "4. Updates the system path to include NGC CLI\n",
    "\n",
    "This enables us to seamlessly access NVIDIA's pre-trained and optimized models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing NGC CLI on the local machine.\n",
    "## Download and install\n",
    "%env CLI=ngccli_cat_linux.zip\n",
    "!mkdir -p $LOCAL_PROJECT_DIR/ngccli\n",
    "\n",
    "# Remove any previously existing CLI installations\n",
    "!rm -rf $LOCAL_PROJECT_DIR/ngccli/*\n",
    "!wget \"https://ngc.nvidia.com/downloads/$CLI\" -P $LOCAL_PROJECT_DIR/ngccli\n",
    "!unzip -u \"$LOCAL_PROJECT_DIR/ngccli/$CLI\" -d $LOCAL_PROJECT_DIR/ngccli/\n",
    "!rm $LOCAL_PROJECT_DIR/ngccli/*.zip \n",
    "os.environ[\"PATH\"]=\"{}/ngccli/ngc-cli:{}\".format(os.getenv(\"LOCAL_PROJECT_DIR\", \"\"), os.getenv(\"PATH\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preparing Output Directory\n",
    "\n",
    "Before downloading the model, we'll create a dedicated directory to store the quantized ONNX model files. This keeps our project organized and ensures we have a clean location for the downloaded assets.\n",
    "\n",
    "The quantized model is critical for efficient deployment on resource-constrained edge devices like the Infineon PSOC EDGE, as it uses INT8 precision instead of FP32, significantly reducing memory footprint and computational requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p !mkdir -p $LOCAL_EXPERIMENT_DIR/quantized_onnx_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Downloading Pre-trained and Optimized PeopleNet Model\n",
    "\n",
    "Now we'll download NVIDIA's pre-trained, pruned, and quantized PeopleNet model from NGC. This model has already undergone several optimization steps:\n",
    "\n",
    "1. **Training** - Trained on large datasets for person detection\n",
    "2. **Pruning** - Removal of redundant connections to reduce model size\n",
    "3. **Quantization** - Precision reduction from FP32 to INT8 \n",
    "\n",
    "These optimizations make it ideal for edge deployment without requiring us to perform the time-consuming training process.\n",
    "\n",
    "The model is downloaded in ONNX format, which is an open standard for representing deep learning models that allows interoperability between different frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ngc registry model download-version \"nvidia/tao/peoplenet:pruned_quantized_decrypted_v2.3.4\" \\\n",
    "    --dest $LOCAL_EXPERIMENT_DIR/quantized_onnx_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Converting the Model for Infineon PSOC EDGE Deployment\n",
    "\n",
    "In this critical step, we'll convert the quantized ONNX model to a format optimized for Infineon PSOC EDGE devices with Arm Ethos-U55 NPU acceleration.\n",
    "\n",
    "The conversion process:\n",
    "1. Uses Infineon's IFX tooling to analyze the model architecture\n",
    "2. Configures the target hardware (Ethos-U55-128 NPU)\n",
    "3. Sets system parameters specific to PSOC EDGE (PSE84_M55_U55_400MHz)\n",
    "4. Optimizes memory usage with SRAM-only configuration\n",
    "5. Preserves INT8 quantization for maximum efficiency\n",
    "\n",
    "This step transforms the general-purpose ONNX model into a highly optimized deployment package specifically for the Infineon PSOC EDGE hardware architecture.\n",
    "\n",
    "**Note:** The input shape parameter [1, 3, 544, 960] represents:\n",
    "- Batch size: 1 (processing one image at a time)\n",
    "- Channels: 3 (RGB color inputs)\n",
    "- Height: 544 pixels\n",
    "- Width: 960 pixels\n",
    "\n",
    "These dimensions must match the expected input for the PeopleNet model and should be maintained in your application deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ifx_tooling import run_ifx_tooling, ModelConversionError\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "qat_onnx_model_path = os.path.join(os.environ['LOCAL_EXPERIMENT_DIR'], \"quantized_onnx_model/peoplenet_vpruned_quantized_decrypted_v2.3.4/resnet34_peoplenet_int8.onnx\")\n",
    "ifx_tooling_output_path = os.path.join(os.environ['LOCAL_EXPERIMENT_DIR'], \"ifx_tooling\")\n",
    "\n",
    "config = {\n",
    "    'vela_accelerator': 'ethos-u55-128',\n",
    "    'vela_system_config': 'PSE84_M55_U55_400MHz',\n",
    "    'vela_memory_mode': 'Sram_Only',\n",
    "    'compress_to_fp16': False,\n",
    "    'vela_ini_file_path': os.path.join(os.environ['LOCAL_PROJECT_DIR'], \"vela.ini\")\n",
    "}\n",
    "\n",
    "try:\n",
    "    output_paths = run_ifx_tooling(\n",
    "        onnx_model_path=qat_onnx_model_path,\n",
    "        input_shape=[1, 3, 544, 960],\n",
    "        output_dir=ifx_tooling_output_path,\n",
    "        config=config\n",
    "    )\n",
    "    print(\"Generated artifacts:\", output_paths)\n",
    "except ModelConversionError as e:\n",
    "    print(f\"Conversion failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Next Steps for Deployment\n",
    "\n",
    "After successfully converting the model, you'll find the deployment artifacts in the `ifx_tooling` output directory. To deploy this model on your Infineon PSOC EDGE device:\n",
    "\n",
    "1. Transfer the generated artifacts to your PSOC EDGE development environment\n",
    "2. Use Infineon ModusToolbox™ to incorporate these files into your application\n",
    "3. Implement pre-processing to format input data correctly (resizing, normalization)\n",
    "4. Add post-processing to interpret model outputs (bounding box rendering, threshold filtering)\n",
    "5. Optimize the frame capture and display pipeline for your specific use case\n",
    "\n",
    "For optimal performance on PSOC EDGE devices:\n",
    "- Consider reducing input resolution for faster inference if your application permits\n",
    "- Implement frame skipping for video inputs to balance performance and power consumption\n",
    "- Use the Arm CMSIS-NN libraries for any additional processing that can't be accelerated by the NPU\n",
    "- Profile your application to identify bottlenecks and optimize accordingly\n",
    "\n",
    "Refer to Infineon's ModusToolbox™ documentation for detailed integration instructions specific to your target hardware."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
