{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PeopleNet deployment for Infineon PSOC&trade; MCU Edge devices\n",
    "\n",
    "This notebook demonstrates how to deploy NVIDIA's pretrained PeopleNet model to Infineon PSOC&trade; Edge MCU devices. PeopleNet is a highly optimized deep learning model for detecting and tracking people in images and video streams, making it ideal for edge applications like occupancy monitoring, crowd analysis, and security systems.\n",
    "\n",
    "By following this workflow, you will learn on how to obtain a quantized PeopleNet model and convert it for efficient execution on Infineon's PSOC&trade; Edge MCU hardware with Arm&reg; Ethos-U55 NPU acceleration.\n",
    "\n",
    "> **Note:** This notebook requires an NVIDIA GPU for the conversion process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment setup\n",
    "\n",
    "Set up the environment variables needed for the workflow. These variables define paths for data, experiments, and specify GPU resources.\n",
    "\n",
    "The following cell configures:\n",
    "- Number of GPUs to use (one is sufficient for deployment)\n",
    "- Working directories for the TAO environment\n",
    "- Local directories for data storage and experiment outputs\n",
    "\n",
    "> **Note:** Ensure to update the `LOCAL_PROJECT_DIR` path to match your system configuration if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up env variables for cleaner command line commands.\n",
    "import os\n",
    "\n",
    "%env NUM_GPUS=1\n",
    "%env USER_EXPERIMENT_DIR=/workspace/tao-experiments/peoplenet_onnx\n",
    "%env DATA_DOWNLOAD_DIR=/workspace/tao-experiments/data\n",
    "\n",
    "# Set this path if you don't run the notebook from the samples directory.\n",
    "# %env NOTEBOOK_ROOT=~/tao-samples/detectnet_v2\n",
    "\n",
    "# Please define this local project directory that needs to be mapped to the TAO docker session.\n",
    "# The dataset expected to be present in $LOCAL_PROJECT_DIR/data, while the results for the steps\n",
    "# in this notebook will be stored at $LOCAL_PROJECT_DIR/detectnet_v2\n",
    "# !PLEASE MAKE SURE TO UPDATE THIS PATH!.\n",
    "\n",
    "os.environ[\"LOCAL_PROJECT_DIR\"] = FIXME\n",
    "\n",
    "os.environ[\"LOCAL_DATA_DIR\"] = os.path.join(\n",
    "    os.getenv(\"LOCAL_PROJECT_DIR\", os.getcwd()),\n",
    "    \"data\"\n",
    ")\n",
    "os.environ[\"LOCAL_EXPERIMENT_DIR\"] = os.path.join(\n",
    "    os.getenv(\"LOCAL_PROJECT_DIR\", os.getcwd()),\n",
    "    \"peoplenet\"\n",
    ")\n",
    "\n",
    "# Make the experiment directory \n",
    "! mkdir -p $LOCAL_EXPERIMENT_DIR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Installing NVIDIA GPU Cloud (NGC) command-line interface (CLI) tool\n",
    "\n",
    "To download the pretrained PeopleNet model, NVIDIA's NGC CLI tool is required. NGC hosts optimized deep learning models, frameworks, and software as follows:\n",
    "\n",
    "- Sets up the environment for NGC CLI installation\n",
    "- Downloads the NGC CLI package\n",
    "- Extracts and configures the tool for use\n",
    "- Updates the system path to include NGC CLI\n",
    "\n",
    "This enables to seamlessly access NVIDIA's pretrained and optimized models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing NGC CLI on the local machine.\n",
    "## Download and install\n",
    "%env CLI=ngccli_cat_linux.zip\n",
    "!mkdir -p $LOCAL_PROJECT_DIR/ngccli\n",
    "\n",
    "# Remove any previously existing CLI installations\n",
    "!rm -rf $LOCAL_PROJECT_DIR/ngccli/*\n",
    "!wget \"https://ngc.nvidia.com/downloads/$CLI\" -P $LOCAL_PROJECT_DIR/ngccli\n",
    "!unzip -u \"$LOCAL_PROJECT_DIR/ngccli/$CLI\" -d $LOCAL_PROJECT_DIR/ngccli/\n",
    "!rm $LOCAL_PROJECT_DIR/ngccli/*.zip \n",
    "os.environ[\"PATH\"]=\"{}/ngccli/ngc-cli:{}\".format(os.getenv(\"LOCAL_PROJECT_DIR\", \"\"), os.getenv(\"PATH\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preparing output directory\n",
    "\n",
    "Before downloading the model, create a dedicated directory to store the quantized ONNX model files to keep the project organized and ensures to have a clean location for the downloaded assets.\n",
    "\n",
    "The quantized model is critical for efficient deployment on resource-constrained edge devices like the Infineon PSOC&trade; Edge MCU, as it uses INT8 precision instead of FP32, significantly reducing memory footprint and computational requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p !mkdir -p $LOCAL_EXPERIMENT_DIR/quantized_onnx_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Downloading pretrained and optimized PeopleNet model\n",
    "\n",
    "Download NVIDIA's pretrained, pruned, and quantized PeopleNet model from NGC. This model has already undergone several optimization steps:\n",
    "\n",
    "- **Training:** Trained on large datasets for person detection\n",
    "- **Pruning:** Removal of redundant connections to reduce model size\n",
    "- **Quantization:** Precision reduction from FP32 to INT8 \n",
    "\n",
    "These optimizations make it ideal for edge deployment without requiring to perform the time-consuming training process.\n",
    "\n",
    "The model is downloaded in the ONNX format, which is an open standard for representing deep learning models that allows interoperability between different frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ngc registry model download-version \"nvidia/tao/peoplenet:pruned_quantized_decrypted_v2.3.4\" \\\n",
    "    --dest $LOCAL_EXPERIMENT_DIR/quantized_onnx_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Converting the model for Infineon PSOC&trade; Edge MCU deployment\n",
    "\n",
    "Convert the quantized ONNX model to a format optimized for Infineon PSOC&trade; Edge MCU devices with Arm&reg; Ethos-U55 NPU acceleration.\n",
    "\n",
    "The conversion process:\n",
    "- Uses Infineon's IFX tooling to analyze the model architecture\n",
    "- Configures the target hardware (Ethos-U55-128 NPU)\n",
    "- Sets system parameters specific to PSOC&trade; Edge MCU (PSE84_M55_U55_400MHz)\n",
    "- Optimizes memory usage with SRAM-only configuration\n",
    "- Preserves INT8 quantization for maximum efficiency\n",
    "\n",
    "This step transforms the general-purpose ONNX model into a highly optimized deployment package specifically for the Infineon PSOC&trade; Edge MCU hardware architecture.\n",
    "\n",
    "> **Note:** The input shape parameter [1, 3, 544, 960] represents:\n",
    "\n",
    "- **Batch size:** 1 (processing one image at a time)\n",
    "- **Channels:** 3 (RGB color inputs)\n",
    "- **Height:** 544 pixels\n",
    "- **Width:** 960 pixels\n",
    "\n",
    "These dimensions must match the expected input for the PeopleNet model and should be maintained in your application deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ifx_tooling import run_ifx_tooling, ModelConversionError\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "qat_onnx_model_path = os.path.join(os.environ['LOCAL_EXPERIMENT_DIR'], \"quantized_onnx_model/peoplenet_vpruned_quantized_decrypted_v2.3.4/resnet34_peoplenet_int8.onnx\")\n",
    "ifx_tooling_output_path = os.path.join(os.environ['LOCAL_EXPERIMENT_DIR'], \"ifx_tooling\")\n",
    "\n",
    "config = {\n",
    "    'vela_accelerator': 'ethos-u55-128',\n",
    "    'vela_system_config': 'PSE84_M55_U55_400MHz',\n",
    "    'vela_memory_mode': 'Sram_Only',\n",
    "    'compress_to_fp16': False,\n",
    "    'vela_ini_file_path': os.path.join(os.environ['LOCAL_PROJECT_DIR'], \"vela.ini\")\n",
    "}\n",
    "\n",
    "try:\n",
    "    output_paths = run_ifx_tooling(\n",
    "        onnx_model_path=qat_onnx_model_path,\n",
    "        input_shape=[1, 3, 544, 960],\n",
    "        output_dir=ifx_tooling_output_path,\n",
    "        config=config\n",
    "    )\n",
    "    print(\"Generated artifacts:\", output_paths)\n",
    "except ModelConversionError as e:\n",
    "    print(f\"Conversion failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Deployment\n",
    "\n",
    "After successfully converting the model, you will find the deployment artifacts in the `ifx_tooling` output directory. To deploy this model on your Infineon PSOC&trade; Edge MCU device:\n",
    "\n",
    "- Transfer the generated artifacts to your PSOC&trade; Edge MCU development environment\n",
    "- Use Infineon ModusToolbox&trade; to incorporate these files into your application\n",
    "- Implement pre-processing to format input data correctly (resizing, normalization)\n",
    "- Add post-processing to interpret model outputs (bounding box rendering, threshold filtering)\n",
    "- Optimize the frame capture and display pipeline for your specific use case\n",
    "\n",
    "For optimal performance on the PSOC&trade; Edge MCU devices:\n",
    "- Consider reducing input resolution for faster inference if your application permits\n",
    "- Implement frame skipping for video inputs to balance performance and power consumption\n",
    "- Use the Arm&reg; CMSIS-NN libraries for any additional processing that cannot be accelerated by the NPU\n",
    "- Profile your application to identify bottlenecks and optimize accordingly\n",
    "\n",
    "See the Infineon [ModusToolbox&trade;](https://www.infineon.com/modustoolbox) documentation for detailed integration instructions specific to your target hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
